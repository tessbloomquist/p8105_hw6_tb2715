---
title: "p8105_hw6_tb2715"
author: "Tess"
date: "11/16/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(purrr)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Question 1
# Read and clean birthweight
```{r}
birthweight_data = read_csv("./data/birthweight.csv")

birthweight_data %>%
  janitor:: clean_names() %>%
   mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )
```

```{r}
# create univariate linear model with family income
fincome_effect = lm(bwt ~ fincome, data = birthweight_data) 

fincome_effect %>%
  broom::tidy() %>%
  knitr::kable(digits = 2)
```
In a simple, unadjusted linear regression model, family monthly income (continuous variable) is a significant predictor of birthweight, such that birthweight increases with increasing family monthly income.

```{r}
#examine if the relationship between income and birthweight is true when including covariates

adjusted_model = lm(bwt ~ fincome + delwt + gaweeks + menarche + malform + pnumlbw + mheight + ppbmi, data = birthweight_data) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 4)

adjusted_model  
```
After adjusting for covariates, the relationships remain significant, except for when including the mother age, mother height, and malformation variable. These variables will be removed for the final linear relationship. 

```{r}
my_adjusted_model = lm(bwt ~ fincome + delwt + gaweeks + pnumlbw + ppbmi, data = birthweight_data) 

my_adjusted_model  %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 4)
```

#Residual and Predictive model
```{r}
residuals = 
  birthweight_data %>%
  add_residuals(my_adjusted_model)

predictive = 
  birthweight_data %>%
  add_predictions(my_adjusted_model)

all_bw_data = 
  merge(residuals, predictive)
```


```{r}
all_bw_data %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() +
  geom_smooth()
```
The created model is not very useful. As the predictive model value increases, the residual value tends to decrease. The covariates used in the model would need to be investigated more thouroughly to try and create a better model that is centered around 0. 

# Compare across additional models
```{r}
#create 2 comparison linear models
main_effects = lm(bwt ~ blength + gaweeks, data = birthweight_data) %>%
  broom::tidy()

main_effects

all_effects = lm(bwt ~ bhead * blength * babysex, data = birthweight_data) %>%
  broom::tidy()

all_effects

```

# Use RMSE to evaluate prediction accuracy
```{r}
cv_df = 
  crossv_mc(birthweight_data, 100)

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
    mutate(my_adjusted_model = map(train, ~lm(bwt ~ fincome + delwt + gaweeks + pnumlbw + ppbmi, data = .x)), 
    main_effects = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    all_effects = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_my_model = map2_dbl(my_adjusted_model, test, ~rmse(model = .x, data = .y)),
    rmse_main_model = map2_dbl(main_effects, test, ~rmse(model = .x, data = .y)),
    rmse_all_model = map2_dbl(all_effects, test, ~rmse(model = .x, data = .y)))
```

Plot RMSE values in 
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
It appears that the best model is the model that accounts for head circumference, length, sex, and all interactions amongst these 3 variables. This is not surprising, as all of these variables have to do with the child itself. "my model" has the largest prediction model... which is not surprising.

## Question 2
# Import weather data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

# Create function to bootstrap
```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}
```

# Run bootstrep
```{r}
boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

boot_straps
```

# Create variables for analysis 
```{r}
bootstrap_rsquared = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    rsquared = map(models, broom:: glance)) %>% 
  select(-strap_sample, -models) %>% 
  unnest() %>%
  select(strap_number, r.squared)

bootstrap_beta = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-strap_sample, -models) %>% 
  unnest() %>%
  select(-std.error, -statistic, -p.value) %>%
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(beta1 = tmin, beta0 = `(Intercept)`) %>% 
  mutate(log = log(beta0*beta1))
```

# Plots of boot straps and density plots(beta log)
```{r}
beta_plot = bootstrap_beta %>%
  ggplot(aes(x = log)) +
  geom_density()

beta_plot
```
The distribution of estimates for the log value has a fairly normal distribution with a small divot at the very top of the peak. There is also a larger tail towards the lower values than higher values which could indicate that large outliers are excluded from the bootstrap more than smaller outliers. 

# Plots of boot straps and density plots (rsquared)
```{r}
r_plot = bootstrap_rsquared %>%
  ggplot(aes(x = r.squared)) +
  geom_density()

r_plot
```
The distribution of estimates for r squared value has a fairly normal distribution with a larger tail towards the lower values than higher values. THis could indicate that large outliers are excluded from the bootstrap more than smaller outliers. 

# Determining 95% CI
```{r}
quantile(pull(bootstrap_beta, log), probs =c(0.025, 0.975))

quantile(pull(bootstrap_rsquared, r.squared), probs =c(0.025, 0.975))
```
The 95% CI for the log estimates ranges from 1.96-2.05. 

The 95% CI for the rsquared estimates ranges from 0.89-0.93. 

### unused code
lm(tmax ~ tmin, data = weather_df) %>%
  broom::glance()

bootstrap_results %>%
  group_by(term) %>%
  summarize(boot_se = sd(estimate)) %>%
  knitr::kable(digits = 3)


boot_straps %>% 
  unnest(strap_sample) %>% 
  ggplot(aes(tmin = tmin, tmax = tmax)) + 
  geom_line(aes(group = strap_number), stat = "smooth", method = "lm", se = FALSE, alpha = .1, color = "blue") +
  geom_point(data = weather_df, alpha = .5)


ggplot %>% 
  ggplot(aes(tmin = tmin, tmax = tmax)) + 
  geom_line(aes(group = strap_number), stat = "smooth", method = "lm", se = FALSE, alpha = .1, color = "blue") +
  geom_point(data = weather_df, alpha = .5)

  group_by(term) %>% 
  summarize(boot_se = sd(estimate))
